{"artificial intelligence": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)\u2014AI that can complete virtually any cognitive task at least as well as a human.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, while raising ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\nGoals:\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\n\nTechniques:\nAI research uses a wide variety of techniques to accomplish the goals above.\n\nApplications:\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO).\n\nEthics:\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.\n\nHistory:\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.\nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important developme", "computing": "Computing is any goal-oriented activity requiring, benefiting from, or creating computing machinery. It includes the study and experimentation of algorithmic processes, and the development of both hardware and software. Computing has scientific, engineering, mathematical, technological, and social aspects. Major computing disciplines include computer engineering, computer science, cybersecurity, data science, information systems, information technology, and software engineering.\nThe term computing is also synonymous with counting and calculating. In earlier times, it was used in reference to the action performed by mechanical computing machines, and before that, to human computers.\n\nHistory:\nThe history of computing is longer than the history of computing hardware and includes the history of methods intended for pen and paper (or for chalk and slate) with or without the aid of tables. Computing is intimately tied to the representation of numbers, though mathematical concepts necessary for computing existed before numeral systems. The earliest known tool for use in computation is the abacus, and it is thought to have been invented in Babylon circa between 2700 and 2300 BC. Abaci, of a more modern design, are still used as calculation tools today.\nThe first recorded proposal for using digital electronics in computing was the 1931 paper \"The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena\" by C. E. Wynn-Williams. Claude Shannon's 1938 paper \"A Symbolic Analysis of Relay and Switching Circuits\" then introduced the idea of using electronics for Boolean algebraic operations.\nThe concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947. In 1953, the University of Manchester built the first transistorized computer, the Manchester Baby. However, early junction transistors were relatively bulky devices that were difficult to mass-produce, which limited them to a number of specialised applications. \nIn 1957, Frosch and Derick were able to manufacture the first silicon dioxide field effect transistors at Bell Labs, the first transistors in which drain and source were adjacent at the surface. Subsequently, a team demonstrated a working MOSFET at Bell Labs 1960. The MOSFET made it possible to build high-density integrated circuits, leading to what is known as the computer revolution or microcomputer revolution.\n\nComputer:\nA computer is a machine that manipulates data according to a set of instructions called a computer program. The program has an executable form that the computer can use directly to execute the instructions. The same program in its human-readable source code form, enables a programmer to study and develop a sequence of steps known as an algorithm. Because the instructions can be carried out in different types of computers, a single set of source instructions converts to machine instructions according to the CPU type.\nThe execution process carries out the instructions in a computer program. Instructions express the computations performed by the computer. They trigger sequences of simple actions on the executing machine. Those actions produce effects according to the semantics of the instructions.\n\nSub-disciplines of computing:\n\n\nResearch and emerging technologies:\nDNA-based computing and quantum computing are areas of active research for both computing hardware and software, such as the development of quantum algorithms. Potential infrastructure for future technologies includes DNA origami on photolithography and quantum antennae for transferring information between ion traps. By 2011, researchers had entangled 14 qubits. Fast digital circuits, including those based on Josephson junctions and rapid single flux quantum technology, are becoming more nearly realizable with the discovery of nanoscale superconductors.\nFiber-optic and photonic (optical) devices, which already have been used to transport data over long distances, are starting to be used by data centers, along with CPU and semiconductor memory components. This allows the separation of RAM from CPU by optical interconnects. IBM has created an integrated circuit with both electronic and optical information processing in one chip. This is denoted CMOS-integrated nanophotonics (CINP). One benefit of optical interconnects is that motherboards, which formerly required a certain kind of system on a chip (SoC), can now move formerly dedicated memory and network controllers off the motherboards, spreading the controllers out onto the rack. This allows standardization of backplane interconnects and motherboards for multiple types of SoCs, which allows more timely upgrades of CPUs.\nAnother field of research is spintronics. Spintronics can provide computing power and storage, without heat buildup. Some research is being done on hybrid chips, which combine photonics and spintronics. There is also research ongoing on combining plasmonics, photonics, and electronics.\n\nSee also:\nArtificial intelligence\nComputational science\nComputational thinking\nComputer algebra\nConfidential computing\nCreative computing\nData-centric computing\nElectronic data processing\nEnthusiast computing\nIndex of history of computing articles\nInstruction set architecture\nLehmer sieve\nLiquid computing\nList of computer term etymologies\nMobile computing\nOutline of computers\nOutline of computing\nScientific computing\nSpatial computing\nUbiquitous computing\nUnconventional computing\nUrban computing\nVirtual reality\n\nReferences:\n\n\nExternal links:\n\nFOLDOC: the Free On-Line Dictionary Of Computing", "cloud computing": "Cloud computing is \"a paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual resources with self-service provisioning and administration on-demand,\" according to ISO.\n\nEssential characteristics:\nIn 2011, the National Institute of Standards and Technology (NIST) identified five \"essential characteristics\" for cloud systems. Below are the exact definitions according to NIST:\n\nOn-demand self-service: \"A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.\"\nBroad network access: \"Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).\"\nResource pooling: \" The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand.\"\nRapid elasticity: \"Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.\"\nMeasured service: \"Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.\nBy 2023, the International Organization for Standardization (ISO) had expanded and refined the list.\n\nHistory:\nThe history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This was a time of exploration and experimentation with ways to make large-scale computing power available to more users through time-sharing, optimizing the infrastructure, platform, and applications, and increasing efficiency for end users.\nThe \"cloud\" metaphor for virtualized services dates to 1994, when it was used by General Magic for the universe of \"places\" that mobile agents in the Telescript environment could \"go\". The metaphor is credited to David Hoffman, a General Magic communications specialist, based on its long-standing use in networking and telecom. The expression cloud computing became more widely known in 1996 when Compaq Computer Corporation drew up a business plan for future computing and the Internet. The company's ambition was to supercharge sales with \"cloud computing-enabled applications\". The business plan foresaw that online consumer file storage would likely be commercially successful. As a result, Compaq decided to sell server hardware to internet service providers.\nIn the 2000s, the application of cloud computing began to take shape with the establishment of Amazon Web Services (AWS) in 2002, which allowed developers to build applications independently. In 2006 Amazon Simple Storage Service, known as Amazon S3, and the Amazon Elastic Compute Cloud (EC2) were released. In 2008 NASA's development of the first open-source software for deploying private and hybrid clouds.\nThe following decade saw the launch of various cloud services. In 2010, Microsoft launched Microsoft Azure, and Rackspace Hosting and NASA initiated an open-source cloud-software project, OpenStack. IBM introduced the IBM SmartCloud framework in 2011, and Oracle announced the Oracle Cloud in 2012. In December 2019, Amazon launched AWS Outposts, a service that extends AWS infrastructure, services, APIs, and tools to customer data centers, co-location spaces, or on-premises facilities.\n\nValue proposition:\nCloud computing can enable shorter time to market by providing pre-configured tools, scalable resources, and managed services, allowing users to focus on their core business value instead of maintaining infrastructure. Cloud platforms can enable organizations and individuals to reduce upfront capital expenditures on physical infrastructure by shifting to an operational expenditure model, where costs scale with usage. Cloud platforms also offer managed services and tools, such as artificial intelligence, data analytics, and machine learning, which might otherwise require significant in-house expertise and infrastructure investment.\nWhile cloud computing can offer cost advantages through effective resource optimization, organizations often face challenges such as unused resources, inefficient configurations, and hidden costs without proper oversight and governance. Many cloud platforms provide cost management tools, such as AWS Cost Explorer and Azure Cost Management, and frameworks like FinOps have emerged to standardize financial operations in the cloud. Cloud computing also facilitates collaboration, remote work, and global service delivery by enabling secure access to data and applications from any location with an internet connection.\nCloud providers offer various redundancy options for core services, such as managed storage and managed databases, though redundancy configurations often vary by service tier. Advanced redundancy strategies, such as cross-region replication or failover systems, typically require explicit configuration and may incur additional costs or licensing fees.\nCloud environments operate under a shared responsibility model, where providers are typically responsible for infrastructure security, physical hardware, and software updates, while customers are accountable for data encryption, identity and access management (IAM), and application-level security. These responsibilities vary depending on the cloud service model\u2014Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS)\u2014with customers typically having more control and responsibility in IaaS environments and progressively less in PaaS and SaaS models, often trading control for convenience and managed services.\n\nFactors influencing the adoption and suitability of cloud computing:\nThe decision to adopt cloud computing or maintain on-premises infrastructure depends on factors such as scalability, cost structure, latency requirements, regulatory constraints, and infrastructure customization.\nOrganizations with variable or unpredictable workloads, limited capital for upfront investments, or a focus on rapid scalability benefit from cloud adoption. Startups, SaaS companies, and e-commerce platforms often prefer the pay-as-you-go operational expenditure (OpEx) model of cloud infrastructure. Additionally, companies prioritizing global accessibility, remote workforce enablement, disaster recovery, and leveraging advanced services such as AI/ML and analytics are well-suited for the cloud. In recent years, some cloud providers have started offering specialized services for high-performance computing and low-latency applications, addressing some use cases previously exclusive to on-premises setups.\nOn the other hand, organizations with strict regulatory requirements, highly predictable workloads, or reliance on deeply integrated legacy systems may find cloud infrastructure less suitable. Businesses in industries like defense, government, or those handling highly sensitive data often favor on-premises setups for greater control and data sovereignty. Additionally, companies with ultra-low latency requirements, such as high-frequency trading (HFT) firms, rely on custom hardware (e.g., FPGAs) and physical proximity to exchanges, which most cloud providers cannot fully replicate despite recent advancements. Similarl", "public cloud": "Cloud computing is \"a paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual resources with self-service provisioning and administration on-demand,\" according to ISO.\n\nEssential characteristics:\nIn 2011, the National Institute of Standards and Technology (NIST) identified five \"essential characteristics\" for cloud systems. Below are the exact definitions according to NIST:\n\nOn-demand self-service: \"A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.\"\nBroad network access: \"Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).\"\nResource pooling: \" The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand.\"\nRapid elasticity: \"Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.\"\nMeasured service: \"Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.\nBy 2023, the International Organization for Standardization (ISO) had expanded and refined the list.\n\nHistory:\nThe history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This was a time of exploration and experimentation with ways to make large-scale computing power available to more users through time-sharing, optimizing the infrastructure, platform, and applications, and increasing efficiency for end users.\nThe \"cloud\" metaphor for virtualized services dates to 1994, when it was used by General Magic for the universe of \"places\" that mobile agents in the Telescript environment could \"go\". The metaphor is credited to David Hoffman, a General Magic communications specialist, based on its long-standing use in networking and telecom. The expression cloud computing became more widely known in 1996 when Compaq Computer Corporation drew up a business plan for future computing and the Internet. The company's ambition was to supercharge sales with \"cloud computing-enabled applications\". The business plan foresaw that online consumer file storage would likely be commercially successful. As a result, Compaq decided to sell server hardware to internet service providers.\nIn the 2000s, the application of cloud computing began to take shape with the establishment of Amazon Web Services (AWS) in 2002, which allowed developers to build applications independently. In 2006 Amazon Simple Storage Service, known as Amazon S3, and the Amazon Elastic Compute Cloud (EC2) were released. In 2008 NASA's development of the first open-source software for deploying private and hybrid clouds.\nThe following decade saw the launch of various cloud services. In 2010, Microsoft launched Microsoft Azure, and Rackspace Hosting and NASA initiated an open-source cloud-software project, OpenStack. IBM introduced the IBM SmartCloud framework in 2011, and Oracle announced the Oracle Cloud in 2012. In December 2019, Amazon launched AWS Outposts, a service that extends AWS infrastructure, services, APIs, and tools to customer data centers, co-location spaces, or on-premises facilities.\n\nValue proposition:\nCloud computing can enable shorter time to market by providing pre-configured tools, scalable resources, and managed services, allowing users to focus on their core business value instead of maintaining infrastructure. Cloud platforms can enable organizations and individuals to reduce upfront capital expenditures on physical infrastructure by shifting to an operational expenditure model, where costs scale with usage. Cloud platforms also offer managed services and tools, such as artificial intelligence, data analytics, and machine learning, which might otherwise require significant in-house expertise and infrastructure investment.\nWhile cloud computing can offer cost advantages through effective resource optimization, organizations often face challenges such as unused resources, inefficient configurations, and hidden costs without proper oversight and governance. Many cloud platforms provide cost management tools, such as AWS Cost Explorer and Azure Cost Management, and frameworks like FinOps have emerged to standardize financial operations in the cloud. Cloud computing also facilitates collaboration, remote work, and global service delivery by enabling secure access to data and applications from any location with an internet connection.\nCloud providers offer various redundancy options for core services, such as managed storage and managed databases, though redundancy configurations often vary by service tier. Advanced redundancy strategies, such as cross-region replication or failover systems, typically require explicit configuration and may incur additional costs or licensing fees.\nCloud environments operate under a shared responsibility model, where providers are typically responsible for infrastructure security, physical hardware, and software updates, while customers are accountable for data encryption, identity and access management (IAM), and application-level security. These responsibilities vary depending on the cloud service model\u2014Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS)\u2014with customers typically having more control and responsibility in IaaS environments and progressively less in PaaS and SaaS models, often trading control for convenience and managed services.\n\nFactors influencing the adoption and suitability of cloud computing:\nThe decision to adopt cloud computing or maintain on-premises infrastructure depends on factors such as scalability, cost structure, latency requirements, regulatory constraints, and infrastructure customization.\nOrganizations with variable or unpredictable workloads, limited capital for upfront investments, or a focus on rapid scalability benefit from cloud adoption. Startups, SaaS companies, and e-commerce platforms often prefer the pay-as-you-go operational expenditure (OpEx) model of cloud infrastructure. Additionally, companies prioritizing global accessibility, remote workforce enablement, disaster recovery, and leveraging advanced services such as AI/ML and analytics are well-suited for the cloud. In recent years, some cloud providers have started offering specialized services for high-performance computing and low-latency applications, addressing some use cases previously exclusive to on-premises setups.\nOn the other hand, organizations with strict regulatory requirements, highly predictable workloads, or reliance on deeply integrated legacy systems may find cloud infrastructure less suitable. Businesses in industries like defense, government, or those handling highly sensitive data often favor on-premises setups for greater control and data sovereignty. Additionally, companies with ultra-low latency requirements, such as high-frequency trading (HFT) firms, rely on custom hardware (e.g., FPGAs) and physical proximity to exchanges, which most cloud providers cannot fully replicate despite recent advancements. Similarl", "Quantum computing": "A quantum computer is a computer that exploits quantum mechanical phenomena. On small scales, physical matter exhibits properties of both particles and waves, and quantum computing takes advantage of this behavior using specialized hardware. Classical physics cannot explain the operation of these quantum devices, and a scalable quantum computer could perform some calculations exponentially faster than any modern \"classical\" computer. Theoretically a large-scale quantum computer could break some widely used encryption schemes and aid physicists in performing physical simulations; however, the current state of the art is largely experimental and impractical, with several obstacles to useful applications.\nThe basic unit of information in quantum computing, the qubit (or \"quantum bit\"), serves the same function as the bit in classical computing. However, unlike a classical bit, which can be in one of two states (a binary), a qubit can exist in a superposition of its two \"basis\" states, a state that is in an abstract sense \"between\" the two basis states. When measuring a qubit, the result is a probabilistic output of a classical bit. If a quantum computer manipulates the qubit in a particular way, wave interference effects can amplify the desired measurement results. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform calculations efficiently and quickly.\nQuantum computers are not yet practical for real-world applications. Physically engineering high-quality qubits has proven to be challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. National governments have invested heavily in experimental research aimed at developing  scalable qubits with longer coherence times and lower error rates. Example implementations include superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single atomic particle using electromagnetic fields).\nIn principle, a classical computer can solve the same computational problems as a quantum computer, given enough time. Quantum advantage comes in the form of time complexity rather than computability, and quantum complexity theory shows that some quantum algorithms are exponentially more efficient than the best-known classical algorithms. A large-scale quantum computer could in theory solve computational problems that are not solvable within a reasonable timeframe for a classical computer. This concept of additional ability has been called \"quantum supremacy\". While such claims have drawn significant attention to the discipline, near-term practical use cases remain limited.\n\nHistory:\nFor many years, the fields of quantum mechanics and computer science formed distinct academic communities. Modern quantum theory developed in the 1920s to explain perplexing physical phenomena observed at atomic scales, and digital computers emerged in the following decades to replace human computers for tedious calculations. Both disciplines had practical applications during World War II; computers played a major role in wartime cryptography, and quantum physics was essential for nuclear physics used in the Manhattan Project.\nAs physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, the fields of quantum mechanics and computer science began to converge. In 1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer.\nWhen digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics, prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation.\nIn a 1984 paper, Charles Bennett and Gilles Brassard applied quantum theory to cryptography protocols and demonstrated that quantum key distribution could enhance information security.\nQuantum algorithms then emerged for solving oracle problems, such as Deutsch's algorithm in 1985, the Bernstein\u2013Vazirani algorithm in 1993, and Simon's algorithm in 1994.\nThese algorithms did not solve practical problems, but demonstrated mathematically that one could gain more information by querying a black box with a quantum state in superposition, sometimes referred to as quantum parallelism.\n\nPeter Shor built on these results with his 1994 algorithm for breaking the widely used RSA and Diffie\u2013Hellman encryption protocols, which drew significant attention to the field of quantum computing. In 1996, Grover's algorithm established a quantum speedup for the widely applicable unstructured search problem. The same year, Seth Lloyd proved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations, validating Feynman's 1982 conjecture.\nOver the years, experimentalists have constructed small-scale quantum computers using trapped ions and superconductors.\nIn 1998, a two-qubit quantum computer demonstrated the feasibility of the technology, and subsequent experiments have increased the number of qubits and reduced error rates.\nIn 2019, Google AI and NASA announced that they had achieved quantum supremacy with a 54-qubit machine, performing a computation that is impossible for any classical computer. However, the validity of this claim is still being actively researched.\n\nQuantum information processing:\nComputer engineers typically describe a modern computer's operation in terms of classical electrodynamics.\nWithin these \"classical\" computers, some components (such as semiconductors and random number generators) may rely on quantum behavior, but these components are not isolated from their environment, so any quantum information quickly decoheres.\nWhile programmers may depend on probability theory when designing a randomized algorithm, quantum mechanical notions like superposition and interference are largely irrelevant for program analysis.\nQuantum programs, in contrast, rely on precise control of coherent quantum systems. Physicists describe these systems mathematically using linear algebra. Complex numbers model probability amplitudes, vectors model quantum states, and matrices model the operations that can be performed on these states. Programming a quantum computer is then a matter of composing operations in such a way that the resulting program computes a useful result in theory and is implementable in practice.\nAs physicist Charlie Bennett describes the relationship between quantum and classical computers,\n\nA classical computer is a quantum computer ... so we shouldn't be asking about \"where do quantum speedups come from?\" We should say, \"well, all computers are quantum. ... Where do classical slowdowns come from?\"\n\nCommunication:\nQuantum cryptography enables new ways to transmit data securely; for example, quantum key distribution uses entangled quantum states to establish secure cryptographic keys. When a sender and receiver exchange quantum states, they can guarantee that an adversary does not intercept the message, as any unauthorized eavesdropper would disturb the delicate quantum system and introduce a detectable change. With appropriate cryptographic protocols, the sender and receiver can thus establish shared private information resistant to eavesdropping.\nModern fiber-optic cables can transmit quantum information over relatively short distances. Ongoing experimental research aims to develop more reliable hardware (such as quantum repeaters), hoping to scale this technology to long-distance quantum networks with end-to-end entanglement. Theoretically, this could enable novel technological applications, such as distributed quantum computing and enhanced quantum sensing.\n\nAlgorithms:\nProgress in finding quantum algorithms typically focuses on this quantum circuit model, though exceptions li", "environmental pollution": "Pollution is the introduction of contaminants into the natural environment that cause harm. Pollution can take the form of any substance (solid, liquid, or gas) or energy (such as radioactivity, heat, sound, or light). Pollutants, the components of pollution, can be either foreign substances/energies or naturally occurring contaminants.\nAlthough environmental pollution can be caused by natural events, the word pollution generally implies that the contaminants have a human source, such as manufacturing, extractive industries, poor waste management, transportation or agriculture. Pollution is often classed as point source (coming from a highly concentrated specific site, such as a factory, mine, construction site), or nonpoint source pollution (coming from a widespread distributed sources, such as microplastics or agricultural runoff).\nMany sources of pollution were unregulated parts of industrialization during the 19th and 20th centuries until the emergence of environmental regulation and pollution policy in the later half of the 20th century. Sites where historically polluting industries released persistent pollutants may have legacy pollution long after the source of the pollution is stopped. Major forms of pollution include air pollution, water pollution, litter, noise pollution, plastic pollution, soil contamination, radioactive contamination, thermal pollution, light pollution, and visual pollution.\nPollution has widespread consequences on human and environmental health, having systematic impact on social and economic systems. In 2019, pollution killed approximately nine million people worldwide (about one in six deaths that year); about three-quarters of these deaths were caused by air pollution. A 2022 literature review found that levels of anthropogenic chemical pollution have exceeded planetary boundaries and now threaten entire ecosystems around the world. Pollutants frequently have outsized impacts on vulnerable populations, such as children and the elderly, and marginalized communities, because polluting industries and toxic waste sites tend to be collocated with populations with less economic and political power. This outsized impact is a core reason for the formation of the environmental justice movement, and continues to be a core element of environmental conflicts, particularly in the Global South.\nBecause of the impacts of these chemicals, local and international countries' policy have increasingly sought to regulate pollutants, resulting in increasing air and water quality standards, alongside regulation of specific waste streams. Regional and national policy is typically supervised by environmental agencies or ministries, while international efforts are coordinated by the UN Environmental Program and other treaty bodies. Pollution mitigation is an important part of all of the Sustainable Development Goals.\n\nDefinitions and types:\nThe term \"pollution\" in the modern environmental sense was rare before the 1860s.  The old sense referred to the desecration of something sacred.  According to Adam Rome\"  To describe what we now call air pollution--i.e., the gaseous, chemical, and metallic by-products of combustion and industrial processes--people usually talked of \"the smoke nuisance.\" There were several variations of that term --\"the smoke problem,\" \"the smoke evil,\" even \"the smoke plague.\"  \nVarious definitions of pollution exist, which may or may not recognize certain types, such as noise pollution or greenhouse gases. The United States Environmental Protection Administration defines pollution as \"Any substances in water, soil, or air that degrade the natural quality of the environment, offend the senses of sight, taste, or smell, or cause a health hazard. The usefulness of the natural resource is usually impaired by the presence of pollutants and contaminants.\" In contrast, the United Nations considers pollution to be the \"presence of substances and heat in environmental media (air, water, land) whose nature, location, or quantity produces undesirable environmental effects.\"\nThe major forms of pollution are listed below along with the particular contaminants relevant to each of them:\n\nAir pollution: the release of chemicals and particulates into the atmosphere. Common gaseous pollutants include carbon monoxide, sulfur dioxide, chlorofluorocarbons (CFCs) and nitrogen oxides produced by industry and motor vehicles. Photochemical ozone and smog are created as nitrogen oxides and hydrocarbons react to sunlight. Particulate matter, or fine dust is characterized by their micrometre size PM10 to PM2.5.\nElectromagnetic pollution: the overabundance of electromagnetic radiation in their non-ionizing form, such as radio and television transmissions, Wi-fi etc. Although there is no demonstrable effect on humans there can be interference with radio-astronomy and effects on safety systems of aircraft and cars.\nLight pollution: includes light trespass, over-illumination and astronomical interference.\nLittering: the criminal throwing of inappropriate man-made objects, unremoved, onto public and private properties.\nNoise pollution: which encompasses roadway noise, aircraft noise, industrial noise as well as high-intensity sonar.\nPlastic pollution: involves the accumulation of plastic products and microplastics in the environment that adversely affects wildlife, wildlife habitat, or humans.\nSoil contamination occurs when chemicals are released by spill or underground leakage. Among the most significant soil contaminants are hydrocarbons, heavy metals, MTBE, herbicides, pesticides and chlorinated hydrocarbons.\nRadioactive contamination, resulting from 20th century activities in atomic physics, such as nuclear power generation and nuclear weapons research, manufacture and deployment. (See alpha emitters and actinides in the environment.)\nThermal pollution, is a temperature change in natural water bodies caused by human influence, such as use of water as coolant in a power plant.\nVisual pollution, which can refer to the presence of overhead power lines, motorway billboards, scarred landforms (as from strip mining), open storage of trash, municipal solid waste or space debris.\nWater pollution, caused by the discharge of industrial wastewater from commercial and industrial waste (intentionally or through spills) into surface waters; discharges of untreated sewage and chemical contaminants, such as chlorine, from treated sewage; and releases of waste and contaminants into surface runoff flowing to surface waters (including urban runoff and agricultural runoff, which may contain chemical fertilizers and pesticides, as well as human feces from open defecation).\n\nNatural causes:\nOne of the most significant natural sources of pollution are volcanoes, which during eruptions release large quantities of harmful gases into the atmosphere. Volcanic gases include carbon dioxide, which can be fatal in large concentrations and contributes to climate change, hydrogen halides which can cause acid rain, sulfur dioxides, which are harmful to animals and damage the ozone layer, and hydrogen sulfides, which are capable of killing humans at concentrations of less than 1 part per thousand. Volcanic emissions also include fine and ultrafine particles which may contain toxic chemicals and substances such as arsenic, lead, and mercury.\nWildfires, which can be caused naturally by lightning strikes, are also a significant source of air pollution. Wildfire smoke contains significant quantities of both carbon dioxide and carbon monoxide, which can cause suffocation. Large quantities of fine particulates are found within wildfire smoke as well, which pose a health risk to animals.\n\nHuman generation:\nMotor vehicle emissions are one of the leading causes of air pollution. China, United States, Russia, India, Mexico, and Japan are the world leaders in air pollution emissions. Principal stationary pollution sources include chemical plants, coal-fired power plants, oil refineries, petroc", "Art of Living": "Art of Living or The Art of Living may refer to:\n\nArt of Living Foundation, a volunteer-based, humanitarian and educational non-governmental organization\nThe Art of Living International Center, Bangalore, India\nArt of Living Center (Los Angeles), U.S.\nThe Art of Living, a long-running radio program and later a book by Norman Vincent Peale\nThe Art of Living (film), a 1965 Spanish drama film\nThe Art of Living, a 1965 book by Dietrich von Hildebrand with Alice von Hildebrand\nThe Art of Living: Peace and Freedom in the Here and Now, a 2017 book by Vietnamese Buddhist monk Th\u00edch Nh\u1ea5t H\u1ea1nh\nArt of Living, a 1993 album and song by The Boomers (band)\nThe Art of Living, a 1967 painting by Ren\u00e9 Magritte\n\nSee also:\nThe Art of Living Long, a 1550 book of Luigi Cornaro\nStoicism", "air pollution": "Air pollution is the presence of substances in the air that are harmful to humans, other living beings or the environment. Pollutants can be gases like ozone or nitrogen oxides or small particles like soot and dust. It affects both outdoor air and indoor air.\nNatural sources of air pollution include wildfires, dust storms, and volcanic eruptions. Indoor air pollution is often caused by the use of firewood or agricultural waste for cooking and heating. Outdoor air pollution comes from some industrial processes, the burning of fossil fuels for electricity and transport, waste management and agriculture. Many of the contributors of local air pollution, especially the burning of fossil fuels, also cause greenhouse gas emissions that cause global warming. However air pollution may limit warming locally.\nAir pollution causes around 7 or 8 million deaths each year. It is a significant risk factor for a number of diseases, including stroke, heart disease, chronic obstructive pulmonary disease (COPD), asthma and lung cancer. Particulate matter is the most deadly, both for indoor and outdoor air pollution. Ozone affects crops, and forests are impacted by the pollution that causes acid rain. Overall, the World Bank has estimated that welfare losses (premature deaths) and productivity losses (lost labour) caused by air pollution cost the world economy over $8 trillion per year.\nVarious technologies and strategies reduce air pollution. Key approaches include clean cookers, improved waste management, industrial scrubbers, electric vehicles and renewable energy. National air quality laws have often been effective, notably the 1956 Clean Air Act in Britain and the 1963 US Clean Air Act. International efforts have had mixed results: the Montreal Protocol almost eliminated harmful ozone-depleting chemicals, while international action on climate change has been less successful.\n\nSources:\n\n\nMajor pollutants:\nAir pollutants can be tiny solid or liquid particles dispersed in the air (called aerosols), or gases. Pollutants are classified as primary or secondary. Primary pollutants are produced directly by a source and remain in the same chemical form after they have been emitted into the atmosphere. Examples include carbon monoxide gas from car exhausts, and sulfur dioxide from factories. Secondary pollutants are not emitted directly. Rather, they form in the air when primary pollutants react with each other or with other parts of the atmosphere. Ground-level ozone is one example of a secondary pollutant. Some pollutants may be both primary and secondary: they are both emitted directly and formed from other primary pollutants.\n\nExposure:\nExposure to air pollution varies widely across the world and across groups. Children, for example, are more exposed because they breathe more rapidly than adults and closer to the ground, where pollution from vehicle exhaust and dust is more concentrated. Similarly, people engaging in strenuous exercise inhale more pollutants than those at rest. On the other hand, people can reduce their exposure by wearing high-quality face masks or by using air purifiers.\nFor some pollutants, low exposure can be seen as safe, whereas other pollutants have negative health effects even at low levels. As evidence has grown that even very low levels of air pollutants hurt human health, the WHO halved its recommended safe limit for particulate matter from 10 \u03bcg/m3 to 5 \u03bcg/m3 in 2021. Under the new guideline, nearly the entire global population\u201497%\u2014is classified as exposed to unsafe levels of PM2.5. The new limit for nitrogen dioxide (NO2) became 75% lower. For all pollutants together, the WHO concluded that 99% of the world population is exposed to harmful air pollution.\nFor some pollutants such as black carbon, traffic related exposures may dominate total exposure despite short exposure times, since high concentrations coincide with proximity to major roads or participation in (motorized) traffic. A large portion of total daily exposure occurs as short peaks of high concentrations.\n\nHealth effects:\nAir pollution is an important risk factor for various diseases, such as COPD (a common lung disease), stroke, heart disease, lung cancer and pneumonia. Indoor air pollution is also associated with cataract. Air pollution has further been linked to brain disorders, such as dementia, depression, anxiety and psychosis.\nPollutants strongly linked to ill health include particulate matter, carbon monoxide, nitrogen dioxide (NO2), ozone (O3), and sulphur dioxide (SO2). Fine particulates are especially damaging, as they can enter the bloodstream via the lungs and reach other organs. Air pollution causes disease by driving inflammation and oxidative stress, suppressing the immune system and by damaging DNA.\nEven at very low levels (under the World Health Organization recommended levels), fine particulates can continue to cause harm. However, according to the WHO, 99% of the world's population lives in areas with air pollution that exceeds WHO recommended levels. People living in poverty, babies and older people are also disproportionately affected by air pollution; pregnancy is also more risky when exposed to air pollution.\n\nSocial and environmental impacts:\n\n\nHistory of air pollution:\nMummified remains of people in Peru, Egypt and Britain show that ancient people in these regions suffered from blackening of the lungs caused by open fires in poorly ventilated homes. Recorded complaints of air pollution go back to the Greek and Roman period. Outdoor air pollution became a problem with the rise of cities, caused by household smoke and by early industrial activities (such as smelting and mining). In particular, lead levels, found in Arctic ice cores, were about ten times higher in the Roman period than in the period before.\n\nMeasurement and monitoring:\n\n\nPollution reduction by sector:\nPollution prevention seeks to prevent pollution such as air pollution and could include adjustments to industrial and business activities such as designing sustainable manufacturing processes (and the products' designs) as well as efforts towards renewable energy transitions.\n\nPolicy and regulation:\n\n\nSee also:\n\n\nReferences:\n\n\nFurther reading:\nCorton C (2015). London Fog: The Biography. Harvard University Press. ISBN 978-0-674-08835-1. Retrieved 8 January 2025.\nFowler D, Brimblecombe P, Burrows J, Heal M, Grennfelt P, Stevenson D, et al. (2020). \"A chronology of global air quality\". Phil. Trans. R. Soc. A. 378 (2183). Bibcode:2020RSPTA.37890314F. doi:10.1098/rsta.2019.0314. PMC 7536029. PMID 32981430.\nGonzalez G (2012). The Politics of Air Pollution: Urban Growth, Ecological Modernization, and Symbolic Inclusion. New York: State University of New York Press. ISBN 978-0-7914-8386-2. Retrieved 8 January 2025.\nHill M (2020). Understanding Environmental Pollution (Fourth ed.). Cambridge, UK: Cambridge University Press. ISBN 978-1-108-43610-6. Retrieved 4 February 2025.\nWoodford C (2021). Breathless: Why Air Pollution Matters \u2013 and How it Affects You. Icon Books. ISBN 978-1-78578-710-2. Retrieved 8 January 2025.\n\nExternal links:\n\nWHO Ambient Air quality database\nGlobal real-time air quality index map\nAir Quality Index (AQI) Basics and Calculator\nEuropean Commission > Environment > Air > Air Quality\nHazardous air pollutants at EPA.gov", "robotics": "Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.\nWithin mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering.\nThe goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.\n\nRobotics aspects:\nRobotics usually combines three aspects of design work to create robot systems:\n\nMechanical construction: a frame, form or shape designed to achieve a particular task. For example, a robot designed to travel across heavy dirt or mud might use caterpillar tracks. Origami inspired robots can sense and analyze in extreme environments. The mechanical aspect of the robot is mostly the creator's solution to completing the assigned task and dealing with the physics of the environment around it. Form follows function.\nElectrical components that power and control the machinery. For example, the robot with caterpillar tracks would need some kind of power to move the tracker treads. That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. Even petrol-powered machines that get their power mainly from petrol still require an electric current to start the combustion process which is why most petrol-powered machines like cars, have batteries. The electrical aspect of robots is used for movement (through motors), sensing (where electrical signals are used to measure things like heat, sound, position, and energy status), and operation (robots need some level of electrical energy supplied to their motors and sensors in order to activate and perform basic operations)\nSoftware. A program is how a robot decides when or how to do something. In the caterpillar track example, a robot that needs to move across a muddy road may have the correct mechanical construction and receive the correct amount of power from its battery, but would not be able to go anywhere without a program telling it to move. Programs are the core essence of a robot, it could have excellent mechanical and electrical construction, but if its program is poorly structured, its performance will be very poor (or it may not perform at all). There are three different types of robotic programs: remote control, artificial intelligence, and hybrid. A robot with remote control programming has a preexisting set of commands that it will only perform if and when it receives a signal from a control source, typically a human being with remote control. It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics. Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects and problems they encounter using their preexisting programming. A hybrid is a form of programming that incorporates both AI and RC functions in them.\n\nApplied robotics:\nAs many robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".\nCurrent and potential applications include:\n\nManufacturing. Robots have been increasingly used in manufacturing since the 1960s. According to the Robotic Industries Association US data, in 2016 the automotive industry was the main customer of industrial robots with 52% of total sales. In the auto industry, they can amount for more than half of the \"labor\". There are even \"lights off\" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003.\nAutonomous transport including airplane autopilot and self-driving cars\nDomestic robots including robotic vacuum cleaners, robotic lawn mowers, dishwasher loading and flatbread baking.\nConstruction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton.\nAutomated mining.\nSpace exploration, including Mars rovers.\nEnergy applications including cleanup of nuclear contaminated areas; and cleaning solar panel arrays.\nMedical robots and Robot-assisted surgery designed and used in clinics.\nAgricultural robots. The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage.\nFood processing. Commercial examples of kitchen automation are Flippy (burgers), Zume Pizza (pizza), Cafe X (coffee), Makr Shakr (cocktails), Frobot (frozen yogurts), Sally (salads), salad or food bowl robots manufactured by Dexai (a Draper Laboratory spinoff, operating on military bases), and integrated food bowl assembly systems manufactured by Spyce Kitchen (acquired by Sweetgreen) and Silicon Valley startup Hyphen. Other examples may include manufacturing technologies based on 3D Food Printing.\nMilitary robots.\nRobot sports for entertainment and education, including Robot combat, Autonomous racing, drone racing, and FIRST Robotics.\n\nMechanical robotics areas:\n\n\nControl robotics areas:\nThe mechanical structure of a robot must be controlled to perform tasks. The control of a robot involves three distinct phases \u2013 perception, processing, and action (robotic paradigms). Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions.\nThe processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.\nAt longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects. Mapping techniques can be used to build maps of the world. Finally, motion planning and oth", "nature of": "Word of the Day \nreign\n Your browser doesn't support HTML5 audio Your browser doesn't support HTML5 audio to be the king or queen of a country Blog \nGaffes and blunders (Different types of mistakes)\n New Words celeb bait \u00a9 Cambridge University Press & Assessment 2025 \u00a9 Cambridge University Press & Assessment 2025 To add the nature of things to a word list please sign up or log in. Add the nature of things to one of your lists below, or create a new one.            \n\n Nature is an inherent character or constitution,[1] particularly of the ecosphere or the universe as a whole. In this general sense nature refers to the laws, elements and phenomena of the physical world, including life. Although humans are part of nature, human activity or humans as a whole are often described as at times at odds, or outright separate and even superior to nature.[2]\n During the advent of modern scientific method in the last several centuries, nature became the passive reality, organized and moved by divine laws.[3][4] With the Industrial Revolution, nature increasingly became seen as the part of reality deprived from intentional intervention: it was hence considered as sacred by some traditions (Rousseau, American transcendentalism) or a mere decorum for divine providence or human history (Hegel, Marx). However, a vitalist vision of nature, closer to the pre-Socratic one, got reborn at the same time, especially after Charles Darwin.[2]\n Within the various uses of the word today, \"nature\" often refers to geology and wildlife. Nature can refer to the general realm of living beings, and in some cases to the processes associated with inanimate objects\u2014the way that particular types of things exist and change of their own accord, such as the weather and geology of the Earth. It is often taken to mean the \"natural environment\" or wilderness\u2014wild animals, rocks, forest, and in general those things that have not been substantially altered by human intervention, or which persist despite human intervention. For example, manufactured objects and human interaction generally are not considered part of nature, unless qualified as, for example, \"human nature\" or \"the whole of nature\". This more traditional concept of natural things that can still be found today implies a distinction between the natural and the artificial, with the artificial being understood as that which has been brought into being by a human consciousness or a human mind. Depending on the particular context, the term \"natural\" might also be distinguished from the unnatural or the supernatural.[2]\n The word nature is borrowed from the Old French nature and is derived from the Latin word natura, or \"essential qualities, innate disposition\", and in ancient times, literally meant \"birth\".[5] In ancient philosophy, natura is mostly used as the Latin translation of the Greek word physis (\u03c6\u03cd\u03c3\u03b9\u03c2), which originally related to the intrinsic characteristics of plants, animals, and other features of the world to develop of their own accord.[6][7]\nThe concept of nature as a whole, the physical universe, is one of several expansions of the original notion;[2] it began with certain core applications of the word \u03c6\u03cd\u03c3\u03b9\u03c2 by pre-Socratic philosophers (though this word had a dynamic dimension then, especially for Heraclitus), and has steadily gained currency ever since.\n Earth is the only planet known to support life, and its natural features are the subject of many fields of scientific researc\n\n\ntype, kind, sort, nature, description, character mean a number of individuals thought of as a group because of a common quality or qualities. type may suggest strong and clearly marked similarity throughout the items included so that each is typical of the group. \n\n\n\n\n\n\n\n         \n                        one of three basic body types \n\n\n\n\n\nkind may suggest natural grouping. \n\n\n\n\n\n\n\n         \n                        a zoo seemingly having animals of every kind \n\n\n\n\n\nsort often suggests some disparagement. \n\n\n\n\n\n\n\n         \n                        the sort of newspaper dealing in sensational stories      \n\n\n\n\n\nnature may imply inherent, essential resemblance rather than obvious or superficial likenesses. \n\n\n\n\n\n\n\n         \n                        two problems of a similar nature \n\n\n\n\n\ndescription implies a group marked by agreement in all details belonging to a type as described or defined. \n\n\n\n\n\n\n\n         \n                        not all acts of that description are actually illegal      \n\n\n\n\n\ncharacter implies a group marked by distinctive likenesses peculiar to the type. \n\n\n\n\n\n\n\n         \n                        research on the subject so far has been of an elementary character \n\n\n\n\n type, kind, sort, nature, description, character mean a number of individuals thought of as a group because of a common quality or qualities.  type may suggest strong and clearly marked similarity throughout the items included so that each is typical of the group. kind may suggest natural grouping. sort often suggests some disparagement. nature may imply inherent, essential resemblance rather than obvious or superficial likenesses. description implies a group marked by agreement in all details belonging to a type as described or defined. character implies a group marked by distinctive likenesses peculiar to the type. \n                Middle English, from Middle French, from Latin natura, from natus, past participle of nasci to be born  \u2014 more at nation  14th century, in the meaning defined at sense 3b \n                                            \u201cNature.\u201d Merriam-Webster.com Dictionary, Merriam-Webster, https://www.merriam-webster.com/dictionary/nature. Accessed 29 Jul. 2025.                                         nature \n                Middle English nature \"normal or essential quality of something, nature,\" from early French nature (same meaning), from Latin natura (same meaning), from natus, past participle of nasci \"to be born\"  \u2014 related to innate, native  Nglish: Translation of nature for Spanish Speakers Britannica.com: Encyclopedia article about nature Subscribe to America's largest dictionary and get thousands more definitions and advanced search\u2014ad free! \nSee Definitions and Examples \u00bb\n   Get Word of the Day daily email! Learn a new word every day. Delivered to your inbox! \u00a9 2025 Merriam-Webster, Incorporated", "nature of living": "One would assume that the meaning of life would never need to be defined. Let\u2019s pose the question to you, \u201cwhat is life?\u201d You would have to think for a while unless you were handed a dictionary. You see, with issues like this, one stumbles over the answer and even when the answer is delivered they tend to be vague. All right, let\u2019s change the question. \u201cWhat are living things?\u201d. This seems easier, doesn\u2019t it? Then let\u2019s tackle it.  The dictionary definition goes something like this: \u201cAn individual form of life, such as a bacterium, protist, fungus, plant or animal consisting of a singular cell or a complex of cells in which cell organelles or organs work together to carry out the various processes of life.\u201d But turn and ask your friend the same question, \u201cwhat are living things?\u201d Chances are he or she will associate life or a living being with movement, that is unless he or she refuses to answer the question. Most of us identify life through movement. When we breathe, our chest moves up and down, it makes it easier to point at a person and call him alive. But what about a leaf? If the colour you look at is green, it is alive. But the conundrum arises when one reminds you that there are plants which exist that aren\u2019t green. So, now what is the solution? There is no definite solution, to be honest. On the safe side, one can assume that if something can reproduce, it can be called alive or a living being. Birds, insects, animals, trees, human beings, are a few examples of living things as they have the same characteristic features, like eating, breathing, reproduction, growth, and development, etc. As opposed to living things, non-living things do not have life. While they do show some similarities compared to living things, they lack sensing capability. For instance, some nonliving things can move, a car or a chair can move, however, they are not living things. You know what are living things. You know why they are called so. Now, there\u2019s something called viruses that are considered to be neither a living thing nor a non-living thing. That is to say, they possess certain characteristics of living things (they tend to infect other organisms) as well as non-living things (viruses cannot reproduce without a host). For more detailed information about Living things, visit BYJU\u2019S. Put your understanding of this concept to test by answering a few MCQs. Click \u2018Start Quiz\u2019 to begin! Select the correct answer and click on the \u201cFinish\u201d buttonCheck your score and answers at the end of the quiz Congrats! Visit BYJU\u2019S for all Biology related queries and study materials Your result is as below \n                                                        Request OTP on\n                                                        \n\n\n\n\nVoice Call\n\n Your Mobile number and Email id will not be published. Required fields are marked * \n\t\t\t\t\t\t\t\t\t\tRequest OTP on\n\t\t\t\t\t\t\t\t\t\t\n\n\n\n\nVoice Call\n\n Website  Post My Comment \n\n Nice Information\nVery useful to me Nice information\nVery useful \nWe live in a very peculiar world. We are surrounded by so many things; some move, and some don\u2019t. Some are born naturally, while others are invented or constructed by us humans. When we are born, we have no sense of the world around us. However, we learn more about it as we grow older. The more we learn about it, the more we understand how intertwined our lives are with this majestic place we call home. Everything we do is directly or indirectly connected to our surroundings. From the air we breathe to the road we walk on, each element plays an important role in our lives. As children, we are taught how to differentiate and group things to be able to better understand and identify them. One such point of differentiation is whether an object or thing is living or non-living in nature. This is one of the most basic forms of differentiation we are taught. Read on further to know some names of living and non living things. The meaning of living things is simply the things around us that are living \u2013 alive and breathing! Anything that has a fixed life cycle is considered to be a living thing. Thank you for reading this post, don't forget to subscribe! Some examples of Living things are:\u00a0\u00a0 From the examples, can you see a pattern?\u00a0\u00a0\u00a0 All the \u2018Living\u2019 things are nothing but organisms that interact with their environment to sustain themselves! Any organism that eats, grows, reproduces and then eventually dies is called a living thing. Natural living things cannot live forever and must eventually perish.\u00a0 \u00a0 Teaching living and non-living things to kindergarten students can be tricky. The best way to go about it is to make a checklist of characteristics to which they can compare objects and then decide whether the object is living on non-living.\u00a0 \u00a0 Below are some characteristics of Living things: \u00a0 Living things are made up of \u2018cells\u2019, which are called the building blocks of life. These \u2018cells\u2019 are properly and systematically organised in a living organism. All living beings are made up of one or more cells.\u00a0\u00a0 A living thing must be able to reproduce. They should be able to create offspring. This can be through both sexual and asexual reproduction. An organism passes on its genetic information to its offspring through reproduction.\u00a0\u00a0\u00a0 All living things grow. We are born a different size, and we die a different size. Other living things, such as plants and fish, grow too!\u00a0\u00a0\u00a0 All living things interact and adjust to their environment. We have warm-blooded and cold-blooded organisms; both have unique ways of adjusting their body temperatures according to their environment. Living beings also respond to stimuli in their environment.\u00a0\u00a0\u00a0 All living organisms require nutrition in some form or another. They require nutrition for growth and survival. For us humans, this means eating food and drinking water, and for plants, this entails making their own food by photosynthesis.\u00a0\u00a0\u00a0 All living things must adapt to their environment to have a better chance of survival. \nAdvertisement From Distinguishing Distinctions to Ethics \nAccessibility Information\n \n                                    \n                                        Living Leadership, San Diego, USA\n \nSearch author on:\n\nPubMed\n\u00a0\nGoogle Scholar\n\n \nPart of the book series:\nBiosemiotics (BSEM, volume 26)\n                     \n\n2217 Accesses \n\n1 Citation \n\n1 \nAltmetric\n \n\n\n\n                                \n                                    This is a preview of subscription content, log in via an institution\n\n\n to check access.\n                             Tax calculation will be finalised at checkout Licence this eBook for your library\n                            \n\n\n\n Institutional subscriptions\n                        \n\n\n This book proposes a bold idea. Living beings are distinguishing distinctions. Single cells and multicellular organisms maintain themselves distinct by drawing distinctions. This is\u00a0what\u00a0organisms are and\u00a0what\u00a0they do. From this starting point, key issues examined range across ontology, epistemology, phenomenology, logic, and ethics. Topics discussed include the origin of life, the nature and purpose of biology, the relation between life and logic, the nature and limits of formal logic, the nature of subjects, the subject-object relation, subject-subject relationships and the deep roots of ethics. The book provides a radical new foundation to think about philosophy and biology and appeals to researchers and students in these fields. It powerfully debunks mechanical thinking about living beings and shows the vast reservoir of insights into aliveness available in the arts and humanities. \u00a0\u00a0 \n                                    \n                                        Daniel Carlos Mayer-Foulkes\n                                    \n                                 Daniel Mayer (Mexico City, 1956) is a researcher in the epistemology of biology, an organizational consultant, and a leadership educator. For decades he has reflected on the nature of organization, both of organisms and of organizations. This is the topic of this book. This project began in the 1980\u2019s during ten years work (four as curator) at\u00a0The Monkey Sanctuary, then a world-renowned center for conservation of Amazon woolly monkeys in the UK, and has continued during his career as a consultant and as an educator. He has read papers on these topics at the\u00a0Annual Lonergan Symposium, at Loyola Marymount University (Los Angeles, CA), at the\u00a0Annual International Gathering in Biosemiotics, and is a regular participant in the\u00a0Leadership for Change\u00a0conferences at the University of San Diego. From 2005 to 2019 he was Adjunct Faculty for the Masters of Science in Organizational Leadership at\u00a0National University, San Diego CA. Founder and CEO of\u00a0Living Leadership\u00a0(livingleadership.online), he designs and implements experiential team methodologies for online teaching based on the group-relations approach. Married to Mexican author Vicky Nizri, they have two children and six grandchil", "Intel": "Intel Corporation is an American multinational corporation and technology company headquartered in Santa Clara, California, and incorporated in Delaware. Intel designs, manufactures, and sells computer components such as central processing units (CPUs) and related products for business and consumer markets. It was the world's third-largest semiconductor chip manufacturer by revenue in 2024 and has been included in the Fortune 500 list of the largest United States corporations by revenue since 2007. It was one of the first companies listed on Nasdaq.\nIntel supplies microprocessors for most manufacturers of computer systems, and is one of the developers of the x86 series of instruction sets found in most personal computers (PCs). It also manufactures chipsets, network interface controllers, flash memory, graphics processing units (GPUs), field-programmable gate arrays (FPGAs), and other devices related to communications and computing. Intel has a strong presence in the high-performance general-purpose and gaming PC market with its Intel Core line of CPUs, whose high-end models are among the fastest consumer CPUs, as well as its Intel Arc series of GPUs.\nIntel was founded on July 18, 1968, by semiconductor pioneers Gordon Moore and Robert Noyce, along with investor Arthur Rock, and is associated with the executive leadership and vision of Andrew Grove. The company was a key component of the rise of Silicon Valley as a high-tech center, as well as being an early developer of static (SRAM) and dynamic random-access memory (DRAM) chips, which represented the majority of its business until 1981. Although Intel created the world's first commercial microprocessor chip\u2014the Intel 4004\u2014in 1971, it was not until the success of the PC in the early 1990s that this became its primary business.\n\nDuring the 1990s, the partnership between Microsoft Windows and Intel, known as \"Wintel\", became instrumental in shaping the PC landscape, and solidified Intel's position on the market. As a result, Intel invested heavily in new microprocessor designs in the mid to late 1990s, fostering the rapid growth of the computer industry. During this period, it became the dominant supplier of PC microprocessors, with a market share of 90%, and was known for aggressive and anti-competitive tactics in defense of its market position, particularly against AMD, as well as a struggle with Microsoft for control over the direction of the PC industry. Since the 2000s and especially since the late 2010s, Intel has faced increasing competition from AMD, which has led to a decline in its dominance and market share in the PC market. Nevertheless, with a 68.4% market share as of 2023, Intel still leads the x86 market by a wide margin.\n\nHistory:\n\n\nProduct and market history:\n\n\nIndustries:\n\n\nCorporate affairs:\n\n\nCorporate identity:\n\n\nCharity:\nIn November 2014, Intel designed a Paddington Bear statue\u2014themed \"Little Bear Blue\"\u2014one of fifty statues created by various celebrities and companies which were located around London. Created prior to the release of the film Paddington, the Intel-designed statue was located outside Framestore in Chancery Lane, London, a British visual-effects company which uses Intel technology for films including Paddington. The statues were then auctioned to raise funds for the National Society for the Prevention of Cruelty to Children (NSPCC).\n\nSponsorships:\nIntel sponsors the Intel Extreme Masters, a series of international esports tournaments. It was also a sponsor for the Formula 1 teams BMW Sauber and Scuderia Ferrari together with AMD, AT&T, Pernod Ricard, Diageo and Vodafone. In 2013, Intel became a sponsor of FC Barcelona. In 2017, Intel became a sponsor of the Olympic Games, lasting from the 2018 Winter Olympics to the 2024 Summer Olympics. In 2024, Intel and Riot Games had an annual sponsorship valued at US$5 million, and one with JD Gaming for US$3.3 million. The company also had a sponsorship with Global Esports.\n\nLitigations and regulatory disputes:\n\n\nProduct issues:\n\n\nSee also:\n\n\nReferences:\n\n\nExternal links:\n\nOfficial website \nIntel companies grouped at OpenCorporates\nIntel on OpenSecrets, a website that tracks and publishes data on campaign finance and lobbying \nBusiness data for Intel Corporation:", "nvidia": "Nvidia Corporation ( en-VID-ee-\u0259) is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang, Chris Malachowsky, and Curtis Priem, it develops graphics processing units (GPUs), system on a chips (SoCs), and application programming interfaces (APIs) for data science, high-performance computing, and mobile and automotive applications.\nOriginally focused on GPUs for video gaming, Nvidia broadened their use into other markets, including artificial intelligence (AI), professional visualization, and supercomputing. The company's product lines include GeForce GPUs for gaming and creative workloads, and professional GPUs for edge computing, scientific research, and industrial applications. As of the first quarter of 2025, Nvidia held a 92% share of the discrete desktop GPU market.\nIn the early 2000s, the company invested over a billion dollars to develop CUDA, a software platform and API that enabled GPUs to run massively parallel programs for a broad range of compute-intensive applications. As a result, as of 2025, Nvidia controlled more than 80% of the market for GPUs used in training and deploying AI models, and provided chips for over 75% of the world's TOP500 supercomputers. The company has also expanded into gaming hardware and services, with products such as the Shield Portable, Shield Tablet, and Shield TV, and operates the GeForce Now cloud gaming service. It also developed the Tegra line of mobile processors for smartphones, tablets, and automotive infotainment systems. \nIn 2023, Nvidia became the seventh U.S. company to reach a US$1 trillion valuation. In 2025, it became the first to surpass US$4 trillion in market capitalization, driven by rising global demand for data center hardware in the midst of the AI boom.\n\nHistory:\n\n\nFabless manufacturing:\nNvidia uses external suppliers for all phases of manufacturing, including wafer fabrication, assembly, testing, and packaging. Nvidia thus avoids most of the investment and production costs and risks associated with chip manufacturing, although it does sometimes directly procure some components and materials used in the production of its products (e.g., memory and substrates). Nvidia focuses its own resources on product design, quality assurance, marketing, and customer support.\n\nCorporate affairs:\n\n\nGPU Technology Conference:\nNvidia's GPU Technology Conference (GTC) is a series of technical conferences held around the world. It originated in 2009 in San Jose, California, with an initial focus on the potential for solving computing challenges through GPUs. In recent years, the conference's focus has shifted to various applications of artificial intelligence and deep learning; including self-driving cars, healthcare, high-performance computing, and Nvidia Deep Learning Institute (DLI) training. GTC 2018 attracted over 8400 attendees. GTC 2020 was converted to a digital event and drew roughly 59,000 registrants. After several years of remote-only events, GTC in March 2024 returned to an in-person format in San Jose, California.\n\nProduct families:\nNvidia's product families include graphics processing units, wireless communication devices, and automotive hardware and software, such as:\n\nGeForce, consumer-oriented graphics processing products\nRTX, professional visual computing graphics processing products (replacing GTX and Quadro)\nNVS, a multi-display business graphics processor\nTegra, a system on a chip series for mobile devices\nTesla, line of dedicated general-purpose GPUs for high-end image generation applications in professional and scientific fields\nnForce, a motherboard chipset created by Nvidia for Intel (Celeron, Pentium and Core 2) and AMD (Athlon and Duron) microprocessors\nGRID, a set of hardware and services by Nvidia for graphics virtualization\nShield, a range of gaming hardware including the Shield Portable, Shield Tablet and Shield TV\nDrive, a range of hardware and software products for designers and manufacturers of autonomous vehicles. The Drive PX-series is a high-performance computer platform aimed at autonomous driving through deep learning, while Driveworks is an operating system for driverless cars.\nBlueField, a range of data processing units, initially inherited from their acquisition of Mellanox Technologies\nDatacenter/server class CPU, codenamed Grace, released in 2023\nDGX, an enterprise platform designed for deep learning applications\nMaxine, a platform providing developers a suite of AI-based conferencing software\n\nOpen-source software support:\nUntil September 23, 2013, Nvidia had not published any documentation for its advanced hardware, meaning that programmers could not write free and open-source device drivers for its products without resorting to reverse engineering.\nInstead, Nvidia provides its own binary GeForce graphics drivers for X.Org and an open-source library that interfaces with the Linux, FreeBSD or Solaris kernels and the proprietary graphics software. Nvidia also provided but stopped supporting an obfuscated open-source driver that only supports two-dimensional hardware acceleration and ships with the X.Org distribution.\nThe proprietary nature of Nvidia's drivers has generated dissatisfaction within free-software communities. In a 2012 talk, Linus Torvalds, in criticism of Nvidia's approach towards Linux, raised his middle finger and stated \"Nvidia, fuck you.\" Some Linux and BSD users insist on using only open-source drivers and regard Nvidia's insistence on providing nothing more than a binary-only driver as inadequate, given that competing manufacturers such as Intel offer support and documentation for open-source developers, and others like AMD release partial documentation and provide some active development.\nNvidia only provides x86/x64 and ARMv7-A versions of their proprietary driver; as a result, features like CUDA are unavailable on other platforms. Some users claim that Nvidia's Linux drivers impose artificial restrictions, like limiting the number of monitors that can be used at the same time, but the company has not commented on these accusations.\nIn 2014, with its Maxwell GPUs, Nvidia started to require firmware by them to unlock all features of its graphics cards.\nOn May 12, 2022, Nvidia announced that they are opensourcing their GPU kernel modules. Support for Nvidia's firmware was implemented in nouveau in 2023, which allows proper power management and GPU reclocking for Turing and newer graphics card generations.\nIn 21 July 2025, Nvidia announce to extend CUDA support to RISC-V.\n\nDeep learning:\nNvidia GPUs are used in deep learning, and accelerated analytics due to Nvidia's CUDA software platform and API which allows programmers to utilize the higher number of cores present in GPUs to parallelize BLAS operations which are extensively used in machine learning algorithms. They were included in many Tesla, Inc. vehicles before Musk announced at Tesla Autonomy Day in 2019 that the company developed its own SoC and full self-driving computer now and would stop using Nvidia hardware for their vehicles. These GPUs are used by researchers, laboratories, tech companies and enterprise companies. In 2009, Nvidia was involved in what was called the \"big bang\" of deep learning, \"as deep-learning neural networks were combined with Nvidia graphics processing units (GPUs)\". That year, the Google Brain team used Nvidia GPUs to create deep neural networks capable of machine learning, where Andrew Ng determined that GPUs could increase the speed of deep learning systems by about 100 times.\n\nInception Program:\nNvidia's Inception Program was created to support start-ups making exceptional advances in the fields of artificial intelligence and data science. Award winners are announced at Nvidia's GTC Conference. In May 2017, the program had 1,300 companies. As of March 2018, there were 2,800 start-ups in the Inception Program. As of August 2021, the program has over 8,500 members in 90 countries, with cumulative funding o", "nvid": "Visit your regional NVIDIA website for local content, pricing, and where to buy partners specific to your country. AI-driven platform for life sciences research and discovery Fully managed end-to-end AI platform on leading clouds Build, customize, and deploy multimodal generative AI Integrate advanced simulation and AI into complex 3D workflows Guide for using NVIDIA NGC private registry with GPU cloud Accelerated, containerized AI models and SDKs Modernizing data centers with AI and accelerated computing Enterprise AI factory for model development and deployment Architecture for data centers that transform data into intelligence A supercomputer purpose-built for AI and HPC Advanced functional safety and security for edge AI Accelerated computing with modular servers Scalable data center infrastructure for high-performance AI Leading platform for autonomous machines and embedded applications Powerful in-vehicle computing for AI-driven autonomous vehicle systems AI-powered computing for innovative medical devices and imaging RTX graphics cards bring game-changing AI capabilities Thinnest and longest lasting RTX laptops, optimized by Max-Q Smooth, tear-free gaming with NVIDIA G-SYNC monitors Neural rendering tech boosts FPS and enhances image quality Advanced platform for full ray tracing and neural rendering Ultimate responsiveness for faster reactions and better aim AI PCs for gaming, creating, productivity and development High performance laptops and desktops, purpose-built for creators RTX-powered cloud gaming. Choose from 3 memberships Optimize gaming, streaming, and AI-powered creativity AI-enhanced voice and video for next-level streams, videos, and calls World-class streaming media performance The engine of the new industrial revolution High performance, scalability, and security for every data center Performance and energy efficiency for endless possibilities RTX graphics cards bring game-changing AI capabilities Accelerating professional AI, graphics, rendering and compute workloads Virtual solutions for scalable, high-performance computing GPU-powered laptops for gamers and creators High performance laptops purpose-built for creators Accelerate professional AI and visual computing from anywhere Accelerated networks for modern workloads Software-defined hardware accelerators for networking, storage, and security Ethernet performance, availability, and ease of use across a wide range of applications High-performance networking for super computers, AI, and cloud data centers Networking software for optimized performance and scalability IO subsystem for modern, GPU-accelerated data centers A Grace Blackwell AI Supercomputer on your desk Accelerate innovation and productivity in AI workflows Powerful AI, graphics, rendering, and compute workloads Accelerate professional AI and visual computing from anywhere                        Simplify AI development with NVIDIA AI Workbench on GPUs Explore NVIDIA's AI models, blueprints, and tools for dev\n\n Nvidia Corporation[a] (/\u025bn\u02c8v\u026adi\u0259/ en-VID-ee-\u0259) is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang, Chris Malachowsky, and Curtis Priem, it develops graphics processing units (GPUs), system on a chips (SoCs), and application programming interfaces (APIs) for data science, high-performance computing, and mobile and automotive applications.[5][6]\n Originally focused on GPUs for video gaming, Nvidia broadened their use into other markets, including artificial intelligence (AI), professional visualization, and supercomputing. The company's product lines include GeForce GPUs for gaming and creative workloads, and professional GPUs for edge computing, scientific research, and industrial applications. As of the first quarter of 2025, Nvidia held a 92% share of the discrete desktop GPU market.[7]\n In the early 2000s, the company invested over a billion dollars to develop CUDA, a software platform and API that enabled GPUs to run massively parallel programs for a broad range of compute-intensive applications.[8][9][10] As a result, as of 2025, Nvidia controlled more than 80% of the market for GPUs used in training and deploying AI models,[8] and provided chips for over 75% of the world's TOP500 supercomputers.[1] The company has also expanded into gaming hardware and services, with products such as the Shield Portable, Shield Tablet, and Shield TV, and operates the GeForce Now cloud gaming service.[11] It also developed the Tegra line of mobile processors for smartphones, tablets, and automotive infotainment systems.[12][13][14]\n In 2023, Nvidia became the seventh U.S. company to reach a US$1 trillion valuation.[15] In 2025, it became the first to surpass US$4 trillion in market capitalization, driven by rising global demand for data center hardware in the midst of the AI boom.[16][17]\n Nvidia was founded on April 5, 1993,[18][19][20] by Jensen Huang (who remains CEO), a Taiwanese-American electrical engineer who was previously the director of CoreWare at LSI Logic and a microprocessor designer at AMD; Chris Malachowsky, an engineer who worked at Sun Microsystems; and Curtis Priem, who was previously a senior staff engineer and graphics chip designer at IBM and Sun Microsystems.[21][22] In late 1992, the three men agreed to start the company in a meeting at a Denny's roadside diner on Berryessa Road in East San Jose.[23][24][25][26]\n At the time, Malachowsky and Priem were frustrated with Sun's management and were looking to leave, but Huang was on \"firmer ground\",[27] in that he was already running his own division at LSI.[24] The three co-founders discussed a vision of the future which was so compelling that Huang decided to leave LSI[27] and become the chief executive officer of their new start-up.[24]\n The three co-founders envisioned  graphics-based processing as the best trajectory for tackling challenges that had eluded general-purpose computing methods.[27] As Huang later explained: \"We als\nVisit your regional NVIDIA website for local content, pricing, and where to buy partners specific to your country. AI-driven platform for life sciences research and discovery Fully managed end-to-end AI platform on leading clouds Explore, test, and deploy AI models and agents Integrate advanced simulation and AI into complex 3D workflows Guide for using NVIDIA NGC private registry with GPU cloud Accelerated, containerized AI models and SDKs Modernizing data centers with AI and accelerated computing Enterprise AI factory for model development and deployment Architecture for data centers that transform data into intelligence A supercomputer purpose-built for AI and HPC Advanced functional safety and security for edge AI Accelerated computing with modular servers Scalable data center infrastructure for high-performance AI Leading platform for autonomous machines and embedded applications Powerful in-vehicle computing for AI-driven autonomous vehicle systems AI-powered computing for innovative medical devices and imaging Explore graphics cards, gaming solutions, AI technology, and more RTX graphics cards bring game-changing AI capabilities Thinnest and longest lasting RTX laptops, optimized by Max-Q Smooth, tear-free gaming with NVIDIA G-SYNC monitors Neural rendering tech boosts FPS and enhances image quality Ultimate responsiveness for faster reactions and better aim AI PCs for gaming, creating, productivity and development High performance laptops and desktops, purpose-built for creators RTX-powered cloud gaming. Choose from 3 memberships Optimize gaming, streaming, and AI-powered creativity AI-enhanced voice and video for next-level streams, videos, and calls World-class streaming media performance The engine of the new industrial revolution High performance, scalability, and security for every data center Performance and energy efficiency for endless possibilities RTX graphics cards bring game-changing AI capabilities Accelerating professional AI, graphics, rendering and compute workloads Virtual solutions for scalable, high-performance computing GPU-powered laptops for gamers and creators High performance laptops purpose-built for creators Accelerate professional AI and visual computing from anywhere Accelerated networks for modern workloads Software-defined hardware accelerators for networking, storage, and security Ethernet performance, availability, and ease of use across a wide range of applications High-performance networking for super computers, AI, and cloud data centers Networking software for optimized performance and scalability IO subsystem for modern, GPU-accelerated data centers Accelerating professional AI, graphics, rendering, and compute workloads A Grace Blackwell AI Supercomputer on your desk The ultimate desktop AI supercomputer powered by NVIDIA Grace Blackwell Accelerate innovation and productivity in AI workflows Powerful AI, graphics, rendering, and compute workloads Accelerate professional AI and visual computing from an", "Everest": "Mount Everest (), known locally as Sagarmatha  in Nepal and Qomolangma in Tibet, is Earth's highest mountain above sea level. It lies in the Mahalangur Himal sub-range of the Himalayas and marks part of the China\u2013Nepal border at its summit. Its height was most recently measured in 2020 by Chinese and Nepali authorities as 8,848.86 m (29,031 ft 8+1\u20442 in).\nMount Everest attracts many climbers, including highly experienced mountaineers. There are two main climbing routes, one approaching the summit from the southeast in Nepal (known as the standard route) and the other from the north in Tibet. While not posing substantial technical climbing challenges on the standard route, Everest presents dangers such as altitude sickness, weather, and wind, as well as hazards from avalanches and the Khumbu Icefall. As of May 2024, 340 people have died on Everest. Over 200 bodies remain on the mountain and have not been removed due to the dangerous conditions.\nClimbers typically ascend only part of Mount Everest's elevation, as the mountain's full elevation is measured from the geoid, which approximates sea level. The closest sea to Mount Everest's summit is the Bay of Bengal, almost 700 km (430 mi) away. To approximate a climb of the entire height of Mount Everest, one would need to start from this coastline, a feat accomplished by Tim Macartney-Snape's team in 1990.\nClimbers usually begin their ascent from base camps above 5,000 m (16,404 ft). The amount of elevation climbed from below these camps varies. On the Tibetan side, most climbers drive directly to the North Base Camp. On the Nepalese side, climbers generally fly into Kathmandu, then Lukla, and trek to the South Base Camp, making the climb from Lukla to the summit about 6,000 m (20,000 ft) in elevation gain.\nThe first recorded efforts to reach Everest's summit were made by British mountaineers. As Nepal did not allow foreigners to enter the country at the time, the British made several attempts on the North Ridge route from the Tibetan side. After the first reconnaissance expedition by the British in 1921 reached 7,000 m (22,966 ft) on the North Col, the 1922 expedition on its first summit attempt marked the first time a human had climbed above 8,000 m (26,247 ft)\nand it also pushed the North Ridge route up to 8,321 m (27,300 ft). On the 1924 expedition George Mallory and Andrew Irvine made a final summit attempt on 8 June but never returned, sparking debate as to whether they were the first to reach the top. Tenzing Norgay and Edmund Hillary made the first documented ascent of Everest in 1953, using the Southeast Ridge route. Norgay had reached 8,595 m (28,199 ft) the previous year as a member of the 1952 Swiss expedition. The Chinese mountaineering team of Wang Fuzhou, Gonpo, and Qu Yinhua made the first reported ascent of the peak from the North Ridge on 25 May 1960.\n\nName:\nMount Everest's Nepali/Sanskrit name is Sagarm\u0101th\u0101 (IAST transcription) or Sagar-Matha (\u0938\u0917\u0930-\u092e\u093e\u0925\u093e, [s\u028c\u0261\u028crmat\u02b0a], lit. \"goddess of the sky\"), which means \"the head in the great blue sky\", being derived from \u0938\u0917\u0930 (sagar), meaning \"sky\", and \u092e\u093e\u0925\u093e (m\u0101th\u0101), meaning \"head\".\nThe Tibetan name for Everest is Qomolangma (\u0f47\u0f7c\u0f0b\u0f58\u0f7c\u0f0b\u0f42\u0fb3\u0f44\u0f0b\u0f58, lit. \"holy mother\"). The name was first recorded (in a Chinese transcription) in the 1721 Kangxi Atlas, issued during the reign of Qing Emperor Kangxi; it first appeared in the West in 1733 as Tchoumour Lancma, on a map prepared by the French geographer D'Anville and based on Kangxi Atlas. The Tibetan name is also popularly romanised as Chomolungma and (in Wylie) as Jo-mo-glang-ma.\nThe official Chinese transcription is \u73e0\u7a46\u6717\u739b\u5cf0 (t \u73e0\u7a46\u6717\u746a\u5cf0), or Zh\u016bm\u00f9l\u01cengm\u01ce F\u0113ng in pinyin. While other Chinese names have been used historically, including Sh\u00e8ngm\u01d4 F\u0113ng (t \u8056\u6bcd\u5cf0, s \u5723\u6bcd\u5cf0, lit. \"holy mother peak\"), these names were largely phased out after the Chinese Ministry of Internal Affairs  issued a decree to adopt a sole name in May 1952. \nThe British geographic survey of 1849 attempted to preserve local names when possible (e.g., Kangchenjunga and Dhaulagiri.) However, Andrew Waugh, the British Surveyor General of India, claimed that he could not find a commonly used local name, and that his search for one had been hampered by the Nepalese and Tibetan policy of exclusion of foreigners. Waugh argued that \u2013 because there were many local names \u2013 it would be difficult to favour one name over all others; he therefore decided that Peak XV should be named after British surveyor Sir George Everest, his predecessor as Surveyor General of India. Everest himself opposed the honour, and told the Royal Geographical Society in 1857 that \"Everest\" could neither be written in Hindi nor pronounced by \"the native of India\". Despite Everest's objections, Waugh's proposed name prevailed, and the Royal Geographical Society officially adopted the name \"Mount Everest\" in 1865. The modern pronunciation of Everest () is different from Sir George's pronunciation of his surname ( EEV-rist).\nIn the late 19th century, many European cartographers incorrectly believed that a native name for the mountain was Gaurishankar, a mountain between Kathmandu and Everest.\n\nSurveys:\n\n\nGeology:\nGeologists have subdivided the rocks comprising Mount Everest into three units called formations. Each formation is separated from the other by low-angle faults, called detachments, along which they have been thrust southward over each other. From the summit of Mount Everest to its base these rock units are the Qomolangma Formation, the North Col Formation, and the Rongbuk Formation.\nThe Qomolangma Formation, also known as the Jolmo Lungama Formation, runs from the summit to the top of the Yellow Band, about 8,600 m (28,200 ft) above sea level. It consists of greyish to dark grey or white, parallel laminated and bedded, Ordovician limestone interlayered with subordinate beds of recrystallised dolomite with argillaceous laminae and siltstone. Gansser first reported finding microscopic fragments of crinoids in this limestone. Later petrographic analysis of samples of the limestone from near the summit revealed them to be composed of carbonate pellets and finely fragmented remains of trilobites, crinoids, and ostracods. Other samples were so badly sheared and recrystallised that their original constituents could not be determined. A thick, white-weathering thrombolite bed that is 60 m (200 ft) thick comprises the foot of the \"Third Step\", and base of the summit pyramid of Everest. This bed, which crops out starting about 70 m (230 ft) below the summit of Mount Everest, consists of sediments trapped, bound, and cemented by the biofilms of micro-organisms, especially cyanobacteria, in shallow marine waters. The Qomolangma Formation is broken up by several high-angle faults that terminate at the low angle normal fault, the Qomolangma Detachment. This detachment separates it from the underlying Yellow Band. The lower five metres of the Qomolangma Formation overlying this detachment are very highly deformed.\nThe bulk of Mount Everest, between 7,000 and 8,600 m (23,000 and 28,200 ft), consists of the North Col Formation, of which the Yellow Band forms the upper part between 8,200 to 8,600 m (26,900 to 28,200 ft). The Yellow Band consists of intercalated beds of Middle Cambrian diopside-epidote-bearing marble, which weathers a distinctive yellowish brown, and muscovite-biotite phyllite and semischist. Petrographic analysis of marble collected from about 8,300 m (27,200 ft) found it to consist as much as five per cent of the ghosts of recrystallised crinoid ossicles. The upper five metres of the Yellow Band lying adjacent to the Qomolangma Detachment is badly deformed. A 5\u201340 cm (2.0\u201315.7 in) thick fault breccia separates it from the overlying Qomolangma Formation.\nThe remainder of the North Col Formation, exposed between 7,000 to 8,200 m (23,000 to 26,900 ft) on Mount Everest, consists of interlayered and deformed schist, phyllite, and minor marble. Between 7,600 and 8,200 m (24,900 and 26,900 ft), t"}